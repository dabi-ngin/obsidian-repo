Tokens are a simple representations of keywords, symbols and user input within the text that is read by a [[Lexers|lexer]] as part of code Interpreting and Compiling. This process is referred to as *lexing* or *tokenization* 

tokens are really cool
